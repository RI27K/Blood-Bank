# -*- coding: utf-8 -*-
"""body fat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w2151hPJLAh4H23Y-tlcIurW_Nq8mcRi

### <b> <span style='color:#eac086'></span> Body Fat Prediction
</b>

This project delves into the fascinating world of data-driven health and fitness analysis, aiming to predict an individual's body fat percentage based on various physical measurements.

In the age of advanced technology and data analytics, understanding how different factors contribute to our health and well-being has become more accessible than ever. The project's primary goal is to explore the effectiveness of different regression techniques in predicting body fat percentage accurately.

Body fat percentage is a measure of the proportion of your total body weight that is composed of fat. It is often used as an indicator of overall health and fitness. Having too much body fat, especially excessive visceral fat (fat stored around organs), can be associated with an increased risk of various health conditions, such as heart disease, diabetes, and metabolic syndrome. On the other hand, having too little body fat can also be unhealthy.

Body fat percentage varies between individuals due to factors such as genetics, age, gender, and lifestyle. It's important to note that body fat percentage is not the same as body mass index (BMI), which is a calculation based on weight and height and is often used to categorize individuals into different weight classes (e.g., underweight, normal weight, overweight, obese).
"""





"""Calipers:

Explanation: Calipers are handheld devices used to measure the thickness of skinfolds at various points on the body.
Method: A person trained in skinfold measurement uses calipers to pinch a small fold of skin and underlying fat at specific anatomical sites. The thickness of this skinfold is then measured with the calipers. These measurements are often taken at multiple locations, and the values are used in formulas to estimate body fat percentage.
Bioelectrical Impedance:

Explanation: Bioelectrical impedance is a method that uses a small electrical current to estimate body composition, including body fat percentage.
Method: A low-level electrical current is sent through the body. Since fat, muscle, and other tissues conduct electricity differently, the impedance (resistance to the flow of the electrical current) can be used to estimate the amount of different tissues in the body. This method is often implemented through scales or handheld devices with built-in sensors.
Comparison:
"""





"""### <b> <span style='color:#eac086'></span> About The Dataset
</b>

**Lists estimates of the percentage of body fat of various body circumference measurements for 252 men.**

### Educational use of the dataset
**This data set can be used to illustrate multiple regression techniques. Accurate measurement of body fat is costly and it is desirable to have easy methods of estimating body fat that are not costly.**

### Content
The variables listed below, from left to right, are:

* Density determined from underwater weighing
* Percent body fat
* Age (years)
* Weight (lbs)
* Height (inches)
* Neck circumference (cm)
* Chest circumference (cm)
* Abdomen 2 circumference (cm)
* Hip circumference (cm)
* Thigh circumference (cm)
* Knee circumference (cm)
* Ankle circumference (cm)
* Biceps (extended) circumference (cm)
* Forearm circumference (cm)
* Wrist circumference (cm)

Each row in the dataset represents an individual with various measurements and characteristics.

These data are used to produce the predictive equations for lean body weight given in the abstract "Generalized body composition prediction equation for men using simple measurement techniques", K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance Research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.
"""
import pandas as pd
df = pd.read_csv("../input/body-fat-prediction-dataset/bodyfat.csv")

"""### <b> <span style='color:RED'></span> Importing the Dataset"""

import pandas as pd
df = pd.read_csv("/content/bodyfat.csv")
df.head()





"""Essential Fat:
Men: 2-5%
Women: 10-13%

Athletes:
Men: 6-24%
Women: 16-30%

Fitness:
Men: 14-17%
Women: 21-24%

Average:
Men: 18-24%
Women: 25-31%

Overweight:
Men: 25% and above
Women: 32% and above
"""





"""### **Data Exploration and Preprocessing**

As we embark on our body fat prediction project, it's crucial to dive into the data and get it ready for analysis. This stage sets the foundation for accurate predictions and meaningful insights. Let's go through the key steps we've taken to understand, clean, and prepare the dataset for our upcoming regression methods.

**1. Data Types**

First things first, we need to ensure that each column is represented in the right data type. Numeric columns should be either float (for decimals) or integer (for whole numbers), and categorical columns should be in string format. This step is like making sure we're speaking the same language with our data.
"""

df.shape

df.info()

"""
**2. Summary Statistics**

To get a quick overview of our numerical columns, we've calculated summary statistics. We've looked at metrics like mean, median, standard deviation, and more. These numbers give us insights into the central tendencies and the spread of our data. It's like taking a snapshot of how our data behaves.
"""



df.describe()



"""
**3. Missing Values and Duplicate Columns**

Missing values can be like gaps in our story. We've carefully checked for missing data in each column. Deciding what to do with them is important â€“ we might need to fill in the gaps (imputation) or consider removing rows with missing values. It's about keeping the integrity of our data intact."""

df.isnull().sum()

df.duplicated().sum()

"""**We have no null and duplicate values in the dataset!**

**3. Data Distribution**

Visualizing data distributions is like seeing the story behind the numbers. We've used histograms and density plots to understand how our data is spread across different ranges. This helps us spot patterns, clusters, and even potential outliers. These visualizations give us a deeper understanding of the shape of our data.
"""

# Histogram of 'BodyFat'
import matplotlib.pyplot as plt
plt.hist(df['BodyFat'], bins=20)
plt.xlabel('BodyFat')
plt.ylabel('Frequency')
plt.show()

"""The resulting histogram will show you how the 'BodyFat' values are distributed across different ranges or bins.


1. **X-Axis (BodyFat Values):** The x-axis of the histogram will represent the range of 'BodyFat' values. This range is divided into bins, and each bin represents a certain interval of 'BodyFat' values.

2. **Y-Axis (Frequency):** The y-axis represents the frequency of occurrences of 'BodyFat' values within each bin. The taller the bar in a particular bin, the higher the frequency of data points with 'BodyFat' values falling within that range.

3. **Bars (Histogram Bars):** The bars in the histogram represent the frequency of data points within each bin. The height of each bar corresponds to the number of data points with 'BodyFat' values falling within the corresponding range.

By looking at the histogram, observe patterns such as whether the distribution is symmetric or skewed, whether there are any outliers, and where the majority of 'BodyFat' values tend to fall.

"""



"""Observations for the histogram

Symmetry and Skewness:

Symmetric Distribution: If the histogram is roughly symmetric around the central value, it indicates that the 'BodyFat' values are balanced on both sides of the mean. The data points are distributed relatively evenly on both sides.


Skewed Distribution: If the histogram is skewed to the left (negative skewness), the majority of 'BodyFat' values are concentrated on the right side of the distribution. If the histogram is skewed to the right (positive skewness), the majority of values are concentrated on the left side.

**So this is SYMMETRIC dataset.**

Outliers on One Side: Outliers are data points that are significantly different from the rest of the data. They can appear as isolated bars that are far from the main bulk of the histogram. Outliers might indicate measurement errors or unique cases.

**OUTLIER is present.**

Spread of Data: The width of the histogram can give you an idea of how spread out the 'BodyFat' values are. A wider histogram indicates higher variability.

**Data is SPREADED.**
"""



"""
**4. Outliers**

Outliers are like the unexpected twists in our story. They can really impact our models. We're keeping an eye out for them and considering whether we need to use techniques like robust regression or data transformations to handle them. It's all about making sure our story stays on track.
"""

# Box plot to identify outliers in 'BodyFat'
plt.boxplot(df['BodyFat'])
plt.ylabel('BodyFat')
plt.show()

"""A box plot is a graphical representation that displays the distribution, central tendency, and potential outliers of a dataset. Here's how to interpret the box plot:

1. **Box (Interquartile Range, IQR):** The box represents the interquartile range (IQR), which is the range between the first quartile (Q1, 25th percentile) and the third quartile (Q3, 75th percentile) of the data. The length of the box indicates the spread of the middle 50% of the data.

2. **Median (Q2, 50th percentile):** The line inside the box represents the median value of the data. It indicates the central value of the data.

3. **Whiskers:** The whiskers extend from the edges of the box to the minimum and maximum data points that are within a certain distance (usually 1.5 times the IQR) from the quartiles. Data points outside this range are considered potential outliers.

4. **Outliers:** Individual data points that lie beyond the whiskers are shown as individual points. These are considered potential outliers.

5. **Plot Context:** The y-axis represents the 'BodyFat' values.

The box plot allows you to visually identify the distribution of 'BodyFat' values, the presence of outliers, and the spread of the data. Outliers are data points that are significantly different from the rest of the data and might warrant further investigation.

The "dot circle" you see outside the range of the whiskers in the box plot represents a potential outlier. Outliers are data points that deviate significantly from the rest of the data and can impact the accuracy and performance of your models.

In the context of the box plot:

- If a data point lies above the upper whisker, it's considered a potential upper outlier.
- If a data point lies below the lower whisker, it's considered a potential lower outlier.
"""



"""
## **Data Analysis for Outliers and Skewness**

**1. Outliers**

Outliers are those exceptional data points that stand out from the crowd. They can be due to measurement errors, data entry mistakes, or even genuine extreme values. Identifying outliers is essential because they can have a disproportionate impact on our models, pulling our predictions in unintended directions.

To analyze outliers, we've been using techniques like visualizations and statistical measures. Visualizations, such as box plots, scatter plots, and histograms, help us visually spot data points that are far from the norm. Additionally, we're using statistical methods like the interquartile range (IQR) and z-scores to quantitatively identify outliers.

**2. Skewness**

Skewness is another aspect that can affect the distribution of our data. A skewed distribution can lead to biased predictions, as our models might give more weight to one side of the data than the other. Positive skewness means the tail is on the right, while negative skewness means the tail is on the left.

We're analyzing skewness using both visualizations and statistical measures like the skewness coefficient. Visualization tools like density plots help us assess the symmetry of the data distribution and understand the extent of skewness.

By examining outliers and skewness, we're ensuring that our data is well-behaved and representative. Handling outliers might involve strategies like transforming the data or using robust regression methods. Addressing skewness could involve applying transformations to make the distribution more symmetric.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
fig, ax = plt.subplots(15, 3, figsize=(15, 40))
continuous_columns = ['Density', 'BodyFat', 'Age', 'Weight', 'Height', 'Neck', 'Chest', 'Abdomen',
                      'Hip', 'Thigh','Knee', 'Ankle', 'Biceps', 'Forearm', 'Wrist']

for index, column in enumerate(continuous_columns):
    sns.histplot(df[column], ax=ax[index, 0], kde=True)
    sns.boxplot(df[column], ax=ax[index, 1])
    stats.probplot(df[column], plot=ax[index, 2])

fig.tight_layout()
fig.subplots_adjust(top=0.95)
plt.suptitle("Analysis for Outliers and Skewness", fontsize=20)



"""**Calculation of Skewness**

Skewness is a crucial measure that helps us understand the symmetry and shape of the distribution of our data. It tells us whether the data is more concentrated on one side of the distribution or if it's more evenly spread out. Let's delve into how we calculate skewness and what it reveals about our data.


- If the skewness is **negative**, it indicates a longer tail on the **left** and a negatively skewed distribution.
- If the skewness is **positive**, it indicates a longer tail on the **right** and a positively skewed distribution.
- A skewness value close to **0** suggests that the distribution is relatively symmetric.

**Impact on Models:**

Understanding skewness is vital because skewed data can impact the performance of our regression models. Models often assume a normal distribution, and extreme skewness can lead to biased predictions. By addressing skewness, we're making sure our data aligns better with the assumptions of our models.

By calculating and interpreting skewness, we're gaining insights into the distribution of our data and making informed decisions about how to handle it.
"""

skewness = df.skew()
sorted_skewness = skewness.sort_values()

print(sorted_skewness)

"""**We see Ankle , Hip , Weight,Height are the most skewed columns**"""



"""


**5. Correlations**

We're curious about how our features relate to each other and especially to the target variable, 'BodyFat.' We've calculated correlations to measure these relationships. This gives us a sense of which features might be more influential in predicting body fat. It's like seeing which pieces of the puzzle might fit together.
"""

import pandas as pd

correlation_matrix = df.corr()
correlation_matrix

"""This will print the correlation coefficients between all pairs of columns in your dataset. Correlation values close to +1 or -1 indicate strong linear relationships between variables and can be an indication of multicollinearity."""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

"""In the heatmap, colors closer to red or blue indicate stronger correlations, while colors closer to white indicate weaker or no correlation.

Multicollinearity is present in the data. like body fat and density are highly correlated.

The diagonal values are all 1.0 since they represent the correlation of a variable with itself.

'Density' and 'BodyFat' have a strong negative correlation of approximately -0.988.

'Weight' and 'Abdomen' have a strong positive correlation of approximately 0.882.

Now to remove multicollinearity we can opt many methods to reduce it , like feature selection in which we can remove a column and combining of variables.

Feature Selection: Identify and remove one or more of the highly correlated variables. Choose the variables that are less important for your analysis or are redundant due to their high correlation with others.

Combine Variables: If the correlated variables are conceptually similar, you might consider creating a composite variable that combines their information. This can help reduce multicollinearity.
"""



"""## **Separation of Dependent and Independent Variables**

In our body fat prediction project, it's essential to distinguish between the dependent variable and the independent variables. This separation helps us clearly define what we're trying to predict and what information we're using to make those predictions.

**Dependent Variable: Body Fat**

The dependent variable, also known as the target variable, is the variable we want to predict. In our case, it's the "BodyFat" column. This is the value we're aiming to estimate based on the information we have about other features.

**Independent Variables: Features**

Independent variables, also referred to as predictor variables or features, are the inputs we'll use to make predictions about the dependent variable. These are the columns other than "BodyFat" that provide information about the individuals' characteristics, such as age, weight, height, and other measurements.

**Why Separate Them?**

By separating the dependent and independent variables, we're setting up a clear framework for our regression models. Our models will learn how changes in the independent variables are related to changes in the dependent variable. This allows us to uncover relationships, understand patterns, and ultimately predict body fat accurately.

**How to Implement It:**

We've already taken the step of splitting the dataset into these two components. Our "X" matrix (independent variables) consists of all columns except "BodyFat," and our "y" vector (dependent variable) consists of the "BodyFat" column itself.

This separation is foundational for the regression methods we'll apply. The models will learn from the relationships between the features and the target variable to make informed predictions about body fat.

---

By distinguishing between the dependent and independent variables, we're shaping our analysis in a way that allows us to unravel the factors influencing body fat and construct predictive models that contribute to our project's success.
"""

X = df.drop(['BodyFat','Density'],axis=1)
y = df['Density']

df.head()



"""## **Feature Engineering and Transformation**

**Body Mass Index (BMI) Calculation:**

We recognize that Body Mass Index (BMI) is a valuable indicator of body composition.

This derived feature, "BMI," provides a comprehensive understanding of how weight and height interact to contribute to body fat estimates.

**Waist-to-Chest Ratio (ACratio) and Hip-to-Thigh Ratio (HTratio):**

To uncover potential patterns related to body proportions, we've introduced two new features: the waist-to-chest ratio (ACratio) and the hip-to-thigh ratio (HTratio). These ratios offer insights into how variations in abdominal size relative to chest size and hip size relative to thigh size might influence body fat.

**Streamlining Features:**

As we incorporate these new features, we've also streamlined our dataset. We recognized that some initial features, such as "Weight," "Height," "Abdomen," "Chest," "Hip," and "Thigh," contain overlapping information. To ensure our models focus on the most relevant data, we've removed these redundant features using the `drop` function.

---
By executing these transformations and feature engineering steps, we're enhancing our dataset's richness and setting the stage for more accurate body fat predictions.
"""

X['BMI'] = 703*(X['Weight'] / (X['Height'] ** 2))
X['ACratio'] = X['Abdomen']/X['Chest']
X['HTratio'] = X['Hip']/X['Thigh']
X.drop(['Weight','Height','Abdomen','Chest','Hip','Thigh'],axis=1,inplace=True)
X.head()

"""**Boosting Prediction and Tackling Multicollinearity**

Introducing three novel columns:

Bmi (Body Mass Index)

Acratio (Abdomen Chest Ratio)

HTratio (Hip Thigh Ratio)

strengthens our predictive accuracy while curbing multicollinearity risks. These transformative features tap into intricate body dynamics, enhancing model insights for precise body fat estimation.
"""



"""### **Removing Outliers**"""

import numpy as np

z = np.abs(stats.zscore(X))

#only keep rows in dataframe with all z-scores less than absolute value of 3
X_clean = X[(z<3).all(axis=1)]
y_clean = y[(z<3).all(axis=1)]
#find how many rows are left in the dataframe
X_clean.shape

"""## **Revisiting Correlations of Newly Formed Columns**

With our dataset refined and enriched through preprocessing, we're now revisiting the correlation analysis specifically focusing on the newly engineered columns. This fresh evaluation sheds light on the intricate relationships these features hold with each other and their potential impact on our body fat prediction models.
"""

# Calculate the correlation matrix
correlation_matrix = X.corr()

# Display the correlation matrix
correlation_matrix
X.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()



"""## **Model Building**"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the models
linear = LinearRegression()
ridge = Ridge()
lasso = Lasso()
elastic = ElasticNet()
decision_tree = DecisionTreeRegressor(random_state=40)
grad = GradientBoostingRegressor()
random_forest = RandomForestRegressor(random_state=40)
bayesian = BayesianRidge()
knn = KNeighborsRegressor()

models = [linear, ridge, lasso, elastic, decision_tree, grad, random_forest, bayesian, knn]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.13, random_state=40)

# Initialize a dictionary to store evaluation scores
scores = {}

for model in models:
    model_name = model.__class__.__name__
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    scores[model_name] = {'R2_score': r2, 'RMSE': rmse}

# Convert the dictionary to a DataFrame for better visualization
scores_df = pd.DataFrame(scores).transpose()
scores_df = scores_df.sort_values(by='R2_score', ascending=False)

scores_df

"""### Observations:
**Regression Model Analysis Summary**


**Top-Performing Models:**
- **GradientBoostingRegressor**: Achieved the highest R2 score of 0.779 and a low RMSE of 0.0097. This model displayed consistent and strong predictive power.
- **RandomForestRegressor**: Demonstrated a competitive R2 score of 0.752 and a reasonable RMSE of 0.0103, suggesting reliable predictive accuracy.
- **LinearRegression**: Showcased a solid R2 score of 0.736 and a RMSE of 0.0106, making it another viable option.

**Promising Models:**
- **BayesianRidge**: Achieved an R2 score of 0.734 and a RMSE of 0.0106, indicating respectable predictive performance.
- **Ridge**: Attained an R2 score of 0.659 and a RMSE of 0.0120, demonstrating moderate predictive capabilities.

**Models Needing Improvement:**
- **DecisionTreeRegressor** and **KNeighborsRegressor**: Achieved R2 scores of 0.491 and 0.420, respectively. These models may benefit from further tuning and optimization.

**Models to Reconsider:**
- **Lasso** and **ElasticNet**: Displayed negative R2 scores and relatively high RMSE values, indicating subpar performance for this dataset.

In conclusion, the **GradientBoostingRegressor** stood out as the most accurate and reliable model, closely followed by the **RandomForestRegressor** and **LinearRegression**. These models demonstrated consistent predictive power and relatively low prediction errors. Further enhancements could involve exploring feature importance, cross-validation, and addressing potential overfitting.
"""



"""### **Applying Cross Validation**"""

from sklearn.model_selection import cross_val_score

# List of models
models = [linear, ridge, lasso, elastic, decision_tree, grad, random_forest, bayesian, knn]

# Perform cross-validation for each model
cv_scores = {}
for model in models:
    model_name = model.__class__.__name__
    scores = cross_val_score(model, X, y, cv=5, scoring='r2')  # Use 5-fold cross-validation
    cv_scores[model_name] = scores

# Convert cross-validation scores to a DataFrame for better visualization
cv_scores_df = pd.DataFrame(cv_scores)

# Calculate mean and standard deviation of cross-validation scores
cv_scores_df['Mean R2'] = cv_scores_df.mean(axis=1)
cv_scores_df['Std Dev'] = cv_scores_df.std(axis=1)

cv_scores_df

"""
- The table contains the cross-validation R2 scores for each fold and each model, along with the mean R2 score and the standard deviation of R2 scores across folds for each model.

- The "Mean R2" column represents the average R2 score across all folds. This is an important metric to consider, as it provides an estimate of the model's overall performance.
- The "Std Dev" column represents the standard deviation of R2 scores across all folds. A lower standard deviation indicates that the model's performance is consistent and stable across different subsets of the data.
- Models with higher mean R2 scores and lower standard deviations are generally preferable, as they indicate better predictive performance and more consistent results.

Based on the provided results, you can observe that the GradientBoostingRegressor and RandomForestRegressor models have the highest mean R2 scores and relatively low standard deviations. These models appear to be performing well in terms of capturing variance in the target variable and maintaining stable performance across different folds.

On the other hand, some models like Lasso, ElasticNet, and SVR seem to have negative R2 scores, which suggests that they are not performing well for this dataset."""



"""**Visualization of Gradient Boosting Regressor Performance**

As we embrace the top-performing Gradient Boosting Regressor, let's bring our analysis to life through a scatter plot that vividly illustrates the alignment between actual and predicted values. This visualization captures the essence of our model's predictive prowess.

**Scatter Plot Insights:**

In the scatter plot, each point represents an observation. The x-axis represents the actual body fat values, while the y-axis showcases the predicted values generated by the Gradient Boosting Regressor. Points closer to the diagonal line signify accurate predictions, as actual and predicted values closely align.

**Interpreting the Plot:**

1. **Clustered Alignment:** A denser cluster of points along the diagonal suggests a strong alignment between actual and predicted values. This signifies the model's accuracy in capturing body fat predictions.

2. **Scattered Deviation:** Points deviating from the diagonal indicate prediction discrepancies. The spread of these points reveals where the model performs well and areas that might benefit from further refinement.

---

Through this captivating scatter plot, we're taking our model's performance from numbers to visuals, gaining a dynamic perspective on the alignment between predicted and actual values. This visualization encapsulates the heart of our body fat prediction endeavor.
"""

y_pred = grad.predict(X_test)
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([1.02, 1.10], [1.02, 1.10], color='black', linestyle='--', linewidth=1)
plt.xlabel("Actual-->")
plt.ylabel("Predicted-->")
plt.title("Actual Vs Predicted")
plt.show()

"""### **Predicting Body Fat: Example Scenario**

Let's embark on a predictive journey by applying our Gradient Boosting Regressor model to a hypothetical example. This scenario demonstrates how our model can estimate body fat based on given input features.

Many body composition equations derive their measure of percent body fat from first determining body density.

Once body density is determined, percent bodyfat (%BF) can be calculated using the Siri equation below :

 % Body Fat = (495 / Body Density) - 450
"""

# Define the input values for prediction
input_values = [[23, 36.2, 37.3, 3.131137, 32.0, 27.4, 17.1, 23.281894, 0.915145, 0.077273]]

# Use the predict function to make predictions
def predict(values):
    density = grad.predict(values)
    fat = ((4.95/density[0]) - 4.5) * 100
    print(f'Density: {density[0]} g/cc\nPercentage Body Fat: {fat} %\n')

# Call the predict function with the input values
predict(input_values)

# Prompt the user to input the values
age = float(input("Enter Age: "))
neck = float(input("Enter Neck Circumference: "))
knee = float(input("Enter Knee Circumference: "))
ankle = float(input("Enter Ankle Circumference: "))
biceps = float(input("Enter Biceps Circumference: "))
forearm = float(input("Enter Forearm Circumference: "))
wrist = float(input("Enter Wrist Circumference: "))
bmi = float(input("Enter BMI: "))
ac_ratio = float(input("Enter Abdomen to Chest Ratio: "))
ht_ratio = float(input("Enter Hip to Thigh Ratio: "))

# Create a list with the input values
input_values = [[age, neck, knee, ankle, biceps, forearm, wrist, bmi, ac_ratio, ht_ratio]]

# Use the predict function to make predictions
predict(input_values)



"""## **Conclusion: Navigating the Landscape of Body Fat Prediction**

In our pursuit of unraveling the complexities of body fat prediction, we've embarked on a journey of data exploration, preprocessing, feature engineering, and regression modeling. This project represents a synergy of rigorous analysis and innovative techniques, all with the goal of enhancing our understanding of body composition.

**Key Milestones:**

1. **Data Understanding and Exploration:** We embarked on our journey by comprehensively understanding our dataset, identifying variables, and assessing data quality. Through summary statistics, distribution visualizations, and correlation analyses, we gained deep insights into the relationships that govern body fat composition.

2. **Preprocessing and Transformation:** Preprocessing played a pivotal role in refining our data. We addressed missing values, handled outliers, and curated meaningful features. Our introduction of BMI, Acratio, and HTratio not only enriched our dataset but also streamlined the impact of multicollinearity.

3. **Regression and Prediction:** Employing a diverse range of regression techniques, we embarked on predicting body fat. Linear Regression, Ridge Regression, Lasso Regression, Elastic Net, and the top-performing Gradient Boosting Regressor lent their distinct methodologies to provide accurate estimations.

4. **Visualization and Interpretation:** Our journey was brought to life through visualizations. From correlation matrices to scatter plots, these visual aids amplified our insights, making the complex relationships within the data accessible and actionable.
f body fat prediction, we're equipped with tools to make informed decisions and contribute to the broader understanding of human physiology. Our journey is ongoing, and we're excited to see where it leads us next.

## **Achievements and Future Directions:**

Throughout our dedicated endeavor, we've achieved significant milestones in our quest to predict body fat with accuracy and precision. Among these accomplishments, one stands out prominently: our top-performing Gradient Boosting Regressor achieving an impressive R-squared score of 0.779.
"""
